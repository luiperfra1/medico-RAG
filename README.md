# ü©∫ Asistente de Documentos M√©dicos (RAG local con LangChain + Ollama)

Este proyecto es una aplicaci√≥n **Streamlit** que permite subir documentos m√©dicos en varios formatos (PDF, DOCX, TXT, Markdown) y hacer preguntas en lenguaje natural.  
El sistema usa **RAG (Retrieval-Augmented Generation)**: busca en tus archivos y genera respuestas usando un modelo local de **Ollama** (ej. `qwen2.5:3b-instruct`).

---

## ‚ú® Funcionalidades
- Subida de documentos m√©dicos (informes, recetas, res√∫menes).  
- Indexado autom√°tico en una base vectorial local (**ChromaDB**).  
- Consultas en lenguaje natural con respuestas generadas por LLM local.  
- Opci√≥n de **reiniciar √≠ndice** (solo borra vectores) o **vaciar todo** (√≠ndice + documentos).  
- Interfaz amigable en navegador con Streamlit.  
- Configuraci√≥n flexible de modelo y embeddings desde `.env`.  

---

## üöÄ Instalaci√≥n

1. Clona el repositorio:
   ```bash
   git clone https://github.com/luiperfra1/medico-RAG.git
   cd medico-RAG
   ```

2. Crea un entorno virtual:
   ```bash
   python -m venv .venv
   # Linux / Mac
   source .venv/bin/activate
   # Windows
   .venv\Scripts\activate
   ```

3. Instala dependencias:
   ```bash
   pip install -r requirements.txt
   ```

4. Crea un archivo `.env` en la ra√≠z con:
   ```env
   OLLAMA_MODEL=qwen2.5:3b-instruct
   EMBED_MODEL=nomic-embed-text
   OLLAMA_BASE_URL=http://localhost:11434  # si usas Ollama local
   ```

5. Aseg√∫rate de tener instalado **Ollama** y haber descargado un modelo:
   ```bash
   ollama pull qwen2.5:3b-instruct
   ```

---

## ‚ñ∂Ô∏è Uso

1. Inicia la app:
   ```bash
   streamlit run app/ui.py
   ```

2. Abre en el navegador:
   ```
   http://localhost:8501
   ```

3. Pasos en la interfaz:
   - **A√±adir documentos**: arrastra tus PDFs, DOCX, TXT o MD.  
   - Pulsa **A√±adir a mi biblioteca** ‚Üí se indexan autom√°ticamente.  
   - En **Preguntar a mi biblioteca**, escribe tu pregunta.  
   - Pulsa **Buscar respuesta** y obtendr√°s la informaci√≥n localizada.  

4. Opciones en la barra lateral:
   - **Reiniciar √≠ndice** ‚Üí limpia la base vectorial (los documentos quedan).  
   - **Vaciar todo** ‚Üí elimina √≠ndice + documentos subidos y reinicia la app.  

---

## üìÇ Estructura del proyecto

```
app/
 ‚îú‚îÄ ui.py          # Interfaz Streamlit
 ‚îú‚îÄ rag_chain.py   # Definici√≥n de la cadena RAG
uploads/           # Archivos subidos (ignorado en git)
vectordb/          # Base vectorial persistente (ignorada en git)
data/              # Carpeta opcional para ingesta manual
requirements.txt
.env.example       # Variables de entorno de ejemplo
README.md
```

---

## ‚ö†Ô∏è Notas importantes
- Los documentos subidos se procesan **localmente** (no se env√≠an a servicios externos).  
- Rendimiento: los modelos grandes en Ollama pueden tardar bastante en CPU. Se recomienda GPU o modelos peque√±os (`phi3:mini`, `qwen2.5:0.5b`).  

---
