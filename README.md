# ğŸ©º Asistente de Documentos MÃ©dicos (RAG local con LangChain + Ollama)

Este proyecto es una aplicaciÃ³n **Streamlit** que permite subir documentos mÃ©dicos en varios formatos (PDF, DOCX, TXT, Markdown) y hacer preguntas en lenguaje natural.  
El sistema estÃ¡ basado en **RAG (Retrieval-Augmented Generation)**: primero busca en tus archivos relevantes y luego genera una respuesta usando un modelo local de **Ollama** (ej. `qwen2.5:3b-instruct`).

---

## âœ¨ Funcionalidades principales
- ğŸ“‚ **Subida de documentos mÃ©dicos** (informes, recetas, resÃºmenesâ€¦).  
- ğŸ” **Indexado automÃ¡tico** en una base vectorial local (**ChromaDB**).  
- ğŸ’¬ **Consultas en lenguaje natural** con respuestas generadas por un LLM local.  
- ğŸ—‘ï¸ **GestiÃ³n del Ã­ndice**:  
  - *Reiniciar Ã­ndice*: borra solo vectores (los archivos permanecen).  
  - *Vaciar todo*: borra vectores + archivos subidos y reinicia la app.  
- ğŸŒ **Interfaz web sencilla** con Streamlit.  
- âš™ï¸ **ConfiguraciÃ³n flexible** de modelo y embeddings mediante `.env`.  

---

## ğŸ“Œ Limitaciones conocidas
- âœ… Ideal para **preguntas concretas** y localizadas (ej.: *â€œÂ¿QuÃ© tratamiento aparece para la hipertensiÃ³n de MarÃ­a LÃ³pez?â€*).  
- âš ï¸ Menos eficaz en **consultas muy generales o contextos largos**: el sistema fragmenta documentos en *chunks* de ~1000 caracteres y solo pasa unos pocos al modelo (recorte de contexto).  
- â³ El rendimiento depende del modelo elegido y de tu hardware:
  - En CPU puede ser lento.  
  - Se recomienda usar GPU o modelos ligeros (`phi3:mini`, `qwen2.5:0.5b`) para mejorar fluidez.  

---

## ğŸš€ InstalaciÃ³n

1. **Clona el repositorio**:
   ```bash
   git clone https://github.com/luiperfra1/medico-RAG.git
   cd medico-RAG
   ```

2. **Crea un entorno virtual**:
   ```bash
   python -m venv .venv
   # Linux / Mac
   source .venv/bin/activate
   # Windows
   .venv\Scripts\activate
   ```

3. **Instala dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Configura variables de entorno** en un archivo `.env` en la raÃ­z:
   ```env
   OLLAMA_MODEL=qwen2.5:3b-instruct
   EMBED_MODEL=nomic-embed-text
   OLLAMA_BASE_URL=http://localhost:11434  # si usas Ollama local
   ```

5. **Instala Ollama y descarga un modelo**:
   ```bash
   ollama pull qwen2.5:3b-instruct
   ollama pull nomic-embed-text
   ```

---

## â–¶ï¸ Uso

1. **Inicia la aplicaciÃ³n**:
   ```bash
   streamlit run app/ui.py
   ```

2. Abre en tu navegador:
   ```
   http://localhost:8501
   ```

3. **Flujo de trabajo**:
   - Arrastra tus documentos (PDF, DOCX, TXT, MD).  
   - Pulsa **AÃ±adir a mi biblioteca** â†’ se indexan automÃ¡ticamente.  
   - Escribe tu pregunta en **Preguntar a mi biblioteca**.  
   - Pulsa **Buscar respuesta** y verÃ¡s la respuesta generada a partir de tus documentos.  

4. **Opciones en la barra lateral**:
   - *Reiniciar Ã­ndice*: limpia solo los vectores.  
   - *Vaciar todo*: borra tambiÃ©n los documentos subidos.  

---

## ğŸ“‚ Estructura del proyecto

```
app/
 â”œâ”€ ui.py          # Interfaz Streamlit (subida, preguntas, opciones)
 â”œâ”€ rag_chain.py   # DefiniciÃ³n de la cadena RAG (prompt, LLM, retriever)
 â”œâ”€ ingest.py      # Script CLI para indexar documentos desde /data
uploads/           # Archivos subidos por el usuario (ignorado en git)
vectordb/          # Base vectorial persistente de Chroma (ignorada en git)
data/              # Carpeta opcional para ingesta manual
requirements.txt
.env.example       # Variables de entorno de ejemplo
README.md
```

---

## âš ï¸ Notas importantes
- Todo el procesamiento se hace **en local**: tus documentos no se envÃ­an a servidores externos.  
- Los modelos grandes de Ollama pueden consumir mucha memoria y tardar en CPU.  
- Para obtener mejores resultados:  
  - Haz preguntas **especÃ­ficas**.  
  - Sube documentos bien estructurados (informes, recetas, resÃºmenes claros).  
  - Usa GPU o modelos pequeÃ±os si buscas rapidez.  

---

