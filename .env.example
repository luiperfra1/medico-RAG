# Modelo de chat en Ollama
OLLAMA_MODEL=qwen2.5:3b-instruct
# O puedes usar:
# OLLAMA_MODEL=llama2:latest

# Modelo de embeddings en Ollama (recomendados por Ollama blog)
EMBED_MODEL=nomic-embed-text
# Alternativas:
# EMBED_MODEL=mxbai-embed-large

# Si tu Ollama no corre en localhost:11434, descomenta y ajusta:
# OLLAMA_BASE_URL=http://127.0.0.1:11434
